#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Sep 20 21:53:08 2017

From: https://spark.apache.org/docs/2.1.0/mllib-decision-tree.html
From: https://github.com/apache/spark/blob/master/python/pyspark/ml/classification.py
From: https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine
From: https://github.com/rukamesh/CarPricePrediction/blob/master/car_price_prediction.py
From: https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/

# Run first preprocessing file

@author: diego
"""
###################################################
# Testing some resources
 
# Use StringIndex to transform to numeric value
# From; https://blog.talentica.com/2017/03/21/handling-categorical-features-in-machine-learning/
from pyspark.ml.feature import StringIndexer
indexer = StringIndexer(inputCol="classificacao", outputCol="classificacaoIndex")
news_indexed = indexer.fit(news_df).transform(news_df)
 
#news_indexed.show(3)
# The more frequent has the less index
#+--------+-------------+--------------------+--------------------+--------------------+------------------+
#|  codigo|classificacao|            conteudo|              resumo|              titulo|classificacaoIndex|
#+--------+-------------+--------------------+--------------------+--------------------+------------------+
#|23152808|     Economia|Os estragos da cr...|Os estragos da cr...|Crise reduz a cla...|               2.0|
#|23152823|     Politica|O ministro-chefe ...|O ministro-chefe ...|Sai nova lista pa...|               0.0|
#|23152825|    Violencia|"Da RedacaoMANAUS...|<br><br>Ainda de ...|<br><br>Quase um ...|               5.0|
#+--------+-------------+--------------------+--------------------+--------------------+------------------+

###################################################
# Get features from dataframe and transform in a vector

from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import Tokenizer
from pyspark.ml.feature import CountVectorizer

#Get sample 15% from data
data_s1 = news_df.sample(False, 0.15, 42)
data_s2 = news_df.sample(False, 0.15, 43)
#data_s1.count(),data_s2.count()
#(106234, 105901)

# Chose sample
data = data_s2

# StringIndex
str_idx_model = StringIndexer(inputCol="classificacao", outputCol="idx_classificacao").fit(data)
data_idx_clas = str_idx_model.transform(data)

# Tokenize
tk_model = Tokenizer(inputCol="conteudo", outputCol="tk_conteudo")
data_tk_cont = tk_model.transform(data_idx_clas)

# Remove stop words
# Count tokens and make avarage setting the vocabSize parameter

# Make with sparse vector

# Count Vector
cv_model = CountVectorizer().setInputCol("tk_conteudo").setOutputCol("cv_tk_conteudo").fit(data_tk_cont)
data_cv_tk_cont = cv_model.transform(data_tk_cont)

###################################################
# Naive Bayes

from pyspark.ml.classification import NaiveBayes
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Divide data
splits = data.randomSplit([0.6, 0.4], 1234)
data_train = splits[0]
data_test = splits[1]

nb = NaiveBayes(smoothing=1.0, modelType="multinomial")
model = nb.fit(data_train)

predictions = model.transform(data_test)
        




















#Decision Tree




