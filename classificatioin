#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Sep 20 21:53:08 2017

From: https://spark.apache.org/docs/2.1.0/mllib-decision-tree.html
From: https://github.com/apache/spark/blob/master/python/pyspark/ml/classification.py
From: https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine
From: https://github.com/rukamesh/CarPricePrediction/blob/master/car_price_prediction.py
From: https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/

# Run first preprocessing file

@author: diego
"""
###################################################
# Testing some resources
 
# Use StringIndex to transform to numeric value
# From; https://blog.talentica.com/2017/03/21/handling-categorical-features-in-machine-learning/
from pyspark.ml.feature import StringIndexer
indexer = StringIndexer(inputCol="classificacao", outputCol="classificacaoIndex")
news_indexed = indexer.fit(news_df).transform(news_df)
 
#news_indexed.show(3)
# The more frequent has the less index
#+--------+-------------+--------------------+--------------------+--------------------+------------------+
#|  codigo|classificacao|            conteudo|              resumo|              titulo|classificacaoIndex|
#+--------+-------------+--------------------+--------------------+--------------------+------------------+
#|23152808|     Economia|Os estragos da cr...|Os estragos da cr...|Crise reduz a cla...|               2.0|
#|23152823|     Politica|O ministro-chefe ...|O ministro-chefe ...|Sai nova lista pa...|               0.0|
#|23152825|    Violencia|"Da RedacaoMANAUS...|<br><br>Ainda de ...|<br><br>Quase um ...|               5.0|
#+--------+-------------+--------------------+--------------------+--------------------+------------------+

###################################################
# Get features from dataframe and transform in a vector

from pyspark.ml.feature import StringIndexer
#from pyspark.ml.feature import CountVectorizer
from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from pyspark.sql.functions import col

data = news_df

# StringIndex
str_idx_model = StringIndexer(inputCol="classificacao", outputCol="idx_classificacao").fit(data)
data_idx_clas = str_idx_model.transform(data)

# Tokenize
tk_model = Tokenizer(inputCol="conteudo", outputCol="tk_conteudo")
data_tk_cont = tk_model.transform(data_idx_clas)

#from pyspark.sql.functions import udf
#from pyspark.sql.types import IntegerType

#countTokens = udf(lambda words: len(words), IntegerType())

hashingTF = HashingTF(inputCol="tk_conteudo", outputCol="v_conteudo", numFeatures=10000)
data_v_cont = hashingTF.transform(data_tk_cont)


idf = IDF(inputCol="v_conteudo", outputCol="features_conteudo")
idfModel = idf.fit(data_v_cont)
data_idf_cont = idfModel.transform(data_v_cont)



#from pyspark.mllib.regression import LabeledPoint

#data_rdd_tmp = data_idf_cont.select(col("idx_classificacao").alias("label"), col("features_conteudo").alias("features")).rdd.map(
#        lambda row: LabeledPoint(row.label, as_old(row.features)))


#from pyspark.mllib.util import MLUtils
#data_df_tmp = MLUtils.saveAsLibSVMFile(data_rdd_tmp, "/home/diego/Documents/Data/label_features_libsvm")

#data_idf_cont.select("idx_classificacao", "features_conteudo").show(1)

# Remove stop words
# Count tokens and make avarage setting the vocabSize parameter

# Make with sparse vector

# Count Vector
#cv_model = CountVectorizer().setInputCol("tk_conteudo").setOutputCol("cv_tk_conteudo").fit(data_tk_cont)
#data_cv_tk_cont = cv_model.transform(data_tk_cont)

###################################################
# Convert to libsvm

# assembler vector
from pyspark.ml.feature import VectorAssembler
from pyspark.sql.functions import col

from pyspark.mllib.linalg import Vectors
from pyspark.mllib.feature import StandardScaler


data_idx_cv = data_cv_tk_cont.select(col("idx_classificacao"), col("cv_tk_conteudo"))

# Get data frame with columns of interesting with numerical value
assembler_tmp = VectorAssembler(inputCols=["cv_tk_conteudo"], outputCol="features")
data_tmp = assembler_tmp.transform(data_idx_cv)


from pyspark.mllib import linalg as mllib_linalg
from pyspark.ml import linalg as ml_linalg

def as_old(v):
    if isinstance(v, ml_linalg.SparseVector):
        return mllib_linalg.SparseVector(v.size, v.indices, v.values)
    if isinstance(v, ml_linalg.DenseVector):
        return mllib_linalg.DenseVector(v.values)
    raise ValueError("Unsupported type {0}".format(type(v)))

# label point
from pyspark.mllib.regression import LabeledPoint

data_rdd_tmp = data_tmp.select(col("idx_classificacao").alias("label"), col("features")).rdd.map(
        lambda row: LabeledPoint(row.label, as_old(row.features)))

from pyspark.mllib.util import MLUtils
data_df_tmp = MLUtils.saveAsLibSVMFile(data_rdd_tmp, "features")

#data_df_tmp = SQLContext.CreateDataFrame("label_features")
#data_df_tmp.write.format("libsvm").save("label_features.txt")


###################################################
# Naive Bayes

from pyspark.ml.classification import NaiveBayes
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.functions import col

data = data_idf_cont.select(col("idx_classificacao").alias("label"), col("features_conteudo").alias("features"))


# Divide data
splits = data.randomSplit([0.7, 0.3], 1234)
data_train = splits[0]
data_test = splits[1]

nb = NaiveBayes(smoothing=1.0, modelType="multinomial")
nv_m = nb.fit(data_train)
predictions = nv_m.transform(data_test)

evaluator_nv = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction",
                                              metricName="accuracy")
accuracy = evaluator_nv.evaluate(predictions)
print("Test set accuracy = " + str(accuracy))
# with data.count() 71113  
# Test set accuracy = 0.6130736216573126 CountVectorizer
# Test set accuracy = 0.6035466679243503 HashingTF + IDF


###################################################
# Random forest

from pyspark.ml.feature import IndexToString, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.sql.functions import col

data = data_cv_tk_cont
assembler = VectorAssembler(inputCols=["cv_tk_conteudo"], outputCol="features")
#data_asm = assembler.transform(data)
#featureIndexer = VectorIndexer(inputCol="cv_tk_conteudo", outputCol="features", maxCategories=4).fit(data)

data_idx_ft = data.select(col("idx_classificacao").alias("label"), col("cv_tk_conteudo"))

#data_sel = data.select(col("classificacao").alias("label"), col("conteudo").alias("features"))
#indexer_input = get_indexer_input(data_sel)

# Divide data
splits = data_idx_ft.randomSplit([0.7, 0.3], 1234)
data_train = splits[0]
data_test = splits[1]

rf = RandomForestClassifier(labelCol="label", featuresCol="features", numTrees=3)

# Convert indexed labels back to original labels.
label_converter = IndexToString(inputCol="prediction", outputCol="prediction_label",
                                labels=str_idx_model.labels)

#feature_indexer = VectorIndexer(inputCol="features", outputCol="idx_features", maxCategories=n_clas).fit(data)

# Chain indexers and forest in a Pipeline
pipeline = Pipeline(stages=[str_idx_model, assembler, rf, label_converter])

# Train model.  This also runs the indexers.
rf_m = pipeline.fit(data_train)

# Make predictions.
predictions = rf_m.transform(data_test)

evaluator_rf = MulticlassClassificationEvaluator(
    labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator_rf.evaluate(predictions)
print("Test Error = %g" % (1.0 - accuracy))

###################################################
# Decison Tree

from pyspark.ml.feature import IndexToString, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.sql.functions import col
from pyspark.ml.classification import DecisionTreeClassifier

data = data_cv_tk_cont

assembler = VectorAssembler(inputCols=["cv_tk_conteudo"], outputCol="features")
#data_asm = assembler.transform(data)

data_idx_ft = data.select(col("idx_classificacao").alias("label"), col("cv_tk_conteudo"))

splits = data_idx_ft.randomSplit([0.7, 0.3], 1234)
data_train = splits[0]
data_test = splits[1]

dt = DecisionTreeClassifier(labelCol="label", featuresCol="features")

pipeline = Pipeline(stages=[str_idx_model, assembler, dt])

# Train model.  This also runs the indexers.
model = pipeline.fit(data_train)

# Make predictions.
predictions = model.transform(data_test)

# Select example rows to display.
predictions.select("prediction", "idx_classificacao", "features").show(5)

# Select (prediction, true label) and compute test error
evaluator = MulticlassClassificationEvaluator















#Decision Tree




