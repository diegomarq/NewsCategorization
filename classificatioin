#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Sep 20 21:53:08 2017

From: https://spark.apache.org/docs/2.1.0/mllib-decision-tree.html
From: https://github.com/apache/spark/blob/master/python/pyspark/ml/classification.py
From: https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine
From: https://github.com/rukamesh/CarPricePrediction/blob/master/car_price_prediction.py
From: https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/

# Run first preprocessing file

@author: diego
"""
###################################################
# Testing some resources
 
# Use StringIndex to transform to numeric value
# From; https://blog.talentica.com/2017/03/21/handling-categorical-features-in-machine-learning/
from pyspark.ml.feature import StringIndexer
indexer = StringIndexer(inputCol="classificacao", outputCol="classificacaoIndex")
news_indexed = indexer.fit(news_df).transform(news_df)
 
#news_indexed.show(3)
# The more frequent has the less index
#+--------+-------------+--------------------+--------------------+--------------------+------------------+
#|  codigo|classificacao|            conteudo|              resumo|              titulo|classificacaoIndex|
#+--------+-------------+--------------------+--------------------+--------------------+------------------+
#|23152808|     Economia|Os estragos da cr...|Os estragos da cr...|Crise reduz a cla...|               2.0|
#|23152823|     Politica|O ministro-chefe ...|O ministro-chefe ...|Sai nova lista pa...|               0.0|
#|23152825|    Violencia|"Da RedacaoMANAUS...|<br><br>Ainda de ...|<br><br>Quase um ...|               5.0|
#+--------+-------------+--------------------+--------------------+--------------------+------------------+

###################################################
# Get features from dataframe and transform in a vector

from pyspark.ml.feature import StringIndexer
#from pyspark.ml.feature import CountVectorizer
from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from pyspark.sql.functions import col
from pyspark.ml.feature import StopWordsRemover

data = news_df

# StringIndex
str_idx_model = StringIndexer(inputCol="classificacao", outputCol="idx_classificacao").fit(data)
data_idx_clas = str_idx_model.transform(data)

# Tokenize
tk_model = Tokenizer(inputCol="conteudo", outputCol="tk_conteudo")
data_tk_cont = tk_model.transform(data_idx_clas)

# Remove stop words
#remover = StopWordsRemover(inputCol="tk_conteudo", outputCol="filtered").loadDefaultStopWords("portuguese")
#data_filt_cont = remover.transform(data_tk_cont)

hashingTF = HashingTF(inputCol="tk_conteudo", outputCol="v_conteudo", numFeatures=10000)
data_v_cont = hashingTF.transform(data_tk_cont)

idf = IDF(inputCol="v_conteudo", outputCol="features_conteudo")
idfModel = idf.fit(data_v_cont)
data_idf_cont = idfModel.transform(data_v_cont)


#from pyspark.mllib.regression import LabeledPoint

#data_rdd_tmp = data_idf_cont.select(col("idx_classificacao").alias("label"), col("features_conteudo").alias("features")).rdd.map(
#        lambda row: LabeledPoint(row.label, as_old(row.features)))


#from pyspark.mllib.util import MLUtils
#data_df_tmp = MLUtils.saveAsLibSVMFile(data_rdd_tmp, "/home/diego/Documents/Data/label_features_libsvm")


# Count Vector
#cv_model = CountVectorizer().setInputCol("tk_conteudo").setOutputCol("cv_tk_conteudo").fit(data_tk_cont)
#data_cv_tk_cont = cv_model.transform(data_tk_cont)

###################################################
# Convert to libsvm

# assembler vector
from pyspark.ml.feature import VectorAssembler
from pyspark.sql.functions import col

from pyspark.mllib.linalg import Vectors
from pyspark.mllib.feature import StandardScaler


data_idx_cv = data_cv_tk_cont.select(col("idx_classificacao"), col("cv_tk_conteudo"))

# Get data frame with columns of interesting with numerical value
assembler_tmp = VectorAssembler(inputCols=["cv_tk_conteudo"], outputCol="features")
data_tmp = assembler_tmp.transform(data_idx_cv)


from pyspark.mllib import linalg as mllib_linalg
from pyspark.ml import linalg as ml_linalg

def as_old(v):
    if isinstance(v, ml_linalg.SparseVector):
        return mllib_linalg.SparseVector(v.size, v.indices, v.values)
    if isinstance(v, ml_linalg.DenseVector):
        return mllib_linalg.DenseVector(v.values)
    raise ValueError("Unsupported type {0}".format(type(v)))

# label point
from pyspark.mllib.regression import LabeledPoint

data_rdd_tmp = data_tmp.select(col("idx_classificacao").alias("label"), col("features")).rdd.map(
        lambda row: LabeledPoint(row.label, as_old(row.features)))

from pyspark.mllib.util import MLUtils
data_df_tmp = MLUtils.saveAsLibSVMFile(data_rdd_tmp, "features")

#data_df_tmp = SQLContext.CreateDataFrame("label_features")
#data_df_tmp.write.format("libsvm").save("label_features.txt")


###################################################
# Naive Bayes

from pyspark.ml.classification import NaiveBayes
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.functions import col

data = data_idf_cont.select(col("idx_classificacao").alias("label"), col("features_conteudo").alias("features"))


# Divide data
splits = data.randomSplit([0.7, 0.3], 1234)
data_train = splits[0]
data_test = splits[1]

nb = NaiveBayes(smoothing=1.0, modelType="multinomial")
nv_m = nb.fit(data_train)
predictions = nv_m.transform(data_test)

evaluator_nv = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction",
                                              metricName="accuracy")
accuracy = evaluator_nv.evaluate(predictions)
print("Test set accuracy = " + str(accuracy))
# with data.count() 71113  
# Test set accuracy = 0.6130736216573126 CountVectorizer
# Test set accuracy = 0.6035466679243503 HashingTF + IDF


###################################################
# Random forest

from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer
from pyspark.ml.evaluation import MulticlassClassificationEvaluator


data = data_idf_cont.select(col("idx_classificacao").alias("label"), col("features_conteudo").alias("features"))

# StringIndex
labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(data)

# VectorIndexer
featureIndexer = VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=29).fit(data)

# Divide
(trainingData, testData) = data.randomSplit([0.7, 0.3])

# Train
rf = RandomForestClassifier(labelCol="indexedLabel", featuresCol="indexedFeatures", numTrees=3)

# Convert indexed labels back to original labels
labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel",
                               labels=labelIndexer.labels)

# Chain indexers and forest in a Pipeline
pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])

# Train model.  This also runs the indexers.
model = pipeline.fit(trainingData)

# Make predictions.
predictions = model.transform(testData)

# Select example rows to display.
predictions.select("predictedLabel", "label", "features").show(5)

# Select (prediction, true label) and compute test error
evaluator = MulticlassClassificationEvaluator(
    labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print("Test Error = %g" % (1.0 - accuracy))

###################################################
# Decison Tree

from pyspark.ml import Pipeline
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.feature import StringIndexer, VectorIndexer
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

data = data_idf_cont.select(col("idx_classificacao").alias("label"), col("features_conteudo").alias("features"))

# StringIndex
labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(data)

# VectorIndexer
featureIndexer = VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=29).fit(data)

# Divide
(trainingData, testData) = data.randomSplit([0.7, 0.3])


# Train a DecisionTree model.
dt = DecisionTreeClassifier(labelCol="indexedLabel", featuresCol="indexedFeatures")

# Chain indexers and tree in a Pipeline
pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])

# Train model.  This also runs the indexers.
model = pipeline.fit(trainingData)

# Make predictions.
predictions = model.transform(testData)

# Select example rows to display.
predictions.select("prediction", "indexedLabel", "features").show(5)

# Select (prediction, true label) and compute test error
evaluator = MulticlassClassificationEvaluator(
    labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print("Test Error = %g " % (1.0 - accuracy))

treeModel = model.stages[2]
# summary only
print(treeModel)

###################################################
# Logistic Regression

from pyspark.ml.classification import LogisticRegression

data = data_idf_cont.select(col("idx_classificacao").alias("label"), col("features_conteudo").alias("features"))

lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)

# Fit the model
lrModel = lr.fit(data)

# Print the coefficients and intercept for multinomial logistic regression
#print("Coefficients: \n" + str(lrModel.coefficientMatrix))
#Coefficients: 
#29 X 10000 CSRMatrix

#print("Intercept: " + str(lrModel.interceptVector))
#Intercept: [3.00949013553,2.53064462247,2.31269089788,1.92272825327,1.4310423319,1.16431295657,0.829351986282,0.761683264317,0.510676702575,0.443808166056,0.407185021423,0.382492843318,0.327323001104,0.212160677052,-0.389206713937,-0.403622431322,-0.413349608273,-0.528941455319,-0.562410037942,-0.632905748776,-0.636971710211,-0.687097571862,-0.783713110333,-1.15675369266,-1.36547289457,-1.4864380763,-1.50082821191,-2.75607887185,-2.94180072449]




